{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Optimization in experimental nuclear physics</center>\n",
    "\n",
    "<center>by Chi-En Teh (Fanurs)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Project: Constraining Skyrme with Bayesian inference on heavy-ion collisions and Gaussian-emulated ImQMD</center>\n",
    "\n",
    "In this report, I would describe how optimization methods could be implemented in my research. More specifically, I will discuss about the research questions that I can ask, improve and validate.\n",
    "\n",
    "As discussed in the project proposal, I plan to adopt Gaussian processes to emulate a heavy-ion collision simulation code called \"Improved Quantum Molecular Dynamics (ImQMD)\". Due to the Monte Carlo nature of ImQMD and the complexity of nuclear collisions, we typically have to repeat the ImQMD simulation for at least one million iterations before the physical observables of interest converge and stabilize. In other words, it takes usually a day up to a week for us to compute just one nuclear collision system. Fortunately, it is believed that the mapping from input space (the Skyrme nuclear equation-of-state) to output space (physical observables) is smooth and continuous. This invites the use of some non-parametric models to \"learn\" or \"emulate\" the simulation codes. Based on the learnings from previous colleagues in my research group, I have decided to use \"Gaussian process\" to achieve this purpose.\n",
    "\n",
    "### Maximum likelihood estimation (MLE)\n",
    "There are many ways to apply Gaussian process. In this project, however, I would only be exploiting its regression capacability. Gaussian process regression (GPR) is essentially a supervised non-parametric learning model. The goal is to update the prior functions (before learning the training data) into some posterior function (post-training) that can be used to infer outputs from input data that have not been fed.\n",
    "\n",
    "Just like in the linear regression model, the \"learning\" or \"optimization\" process is done by minimizing the $\\chi^2$. Gaussian process, on the other hand, aims to maximize the likelihood. This exact implementation is often done using some algorithm based on the idea of gradient descent, even though mostly in this project, I would be using existing libraries such as `scikit-learn`, `GPflow`, `PyMC3`, etc. [1]. An benchmarking of their performances would be studied for deciding the choice of library.\n",
    "\n",
    "### Parallel computation and HPCC\n",
    "There are at least two parts in this project that can enjoy the speed-ups by parallel computation and HPCC. The first part if the generation of training data using ImQMD. The ImQMD codes are all written in fortran, and a \"submission\" typically includes at least a million independent nuclear collisions that are being simulated. This allows a natural way to split up the jobs and submit them into different cores of HPCC. This step has has already been taken care by some script written by previous students in my research group. If given enough time, I would like to understand and possibly improve the procedures.\n",
    "\n",
    "The second part involves the use of multi-core computation to optimization the learning of Gaussian process and Bayesian inference. When doing Bayesian inference, we estimate the posterior by Bayes theorem, i.e.\n",
    "$$p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\ .$$\n",
    "In most applications, both $p(x|theta)$ and $p(\\theta)$ can easily be estimated, whereas the \"evidence term\",\n",
    "$$p(x) = \\int d\\theta \\ p(x|\\theta)p(\\theta) \\ , $$\n",
    "can be challenging. One way to make the calculation of \"evidence\" more practical is to invoke Markov Chain Monte Carlo (MCMC) and parallel computing.\n",
    "\n",
    "### Hyperparameters\n",
    "Last but no least, as in most machine learning algorithms, there always involve some hyperparameters that need to be specified prior to trainings. In Gaussian process, the choice of kernel or covariance is typically made by trials and errors.\n",
    "\n",
    "---\n",
    "# References\n",
    "\n",
    "[1] https://blog.dominodatalab.com/fitting-gaussian-process-models-python/<br>\n",
    "[2] https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
